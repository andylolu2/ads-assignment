{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0b1e4a6f",
      "metadata": {
        "id": "0b1e4a6f"
      },
      "source": [
        "# Assessment for Advanced Data Science\n",
        "\n",
        "## Christian Cabrera, Carl Henrik Ek and Neil D. Lawrence\n",
        "\n",
        "### 29th October 2021\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "34e69a76",
      "metadata": {},
      "source": [
        "## Introduction"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d7856d1b",
      "metadata": {
        "id": "d7856d1b"
      },
      "source": [
        "Welcome to the course assessment for the Advanced Data Science unit. In this assessment you will build a prediction system for UK house prices.\n",
        "\n",
        "Your prediction system will be based on data from the UK Price Paid data available [here](https://www.gov.uk/government/statistical-data-sets/price-paid-data-downloads). By combining this data with the UK Office for National Statistics data on the latitude/longitude of postcodes (available [here](https://www.getthedata.com/open-postcode-geo)) you will have a record of house prices and their approximate latitude/longitude. Due to the size of these data you will use a relational database to handle them.\n",
        "\n",
        "To make predictions of the house price you will augment your data with information obtained from Open Street Map: an open license source of mapping information. You will use the techniques you have learnt in the course to indentify and incorporate useful features for house price prediction.\n",
        "\n",
        "Alongside your implementation you will provide a short repository overview describing how you have implemented the different parts of the project and where you have placed those parts in your code repository. You will submit your code alongside a version of this notebook that will allow your examiner to understand and reconstruct the thinking behind your analysis. This notebook is structured to help you in creating that description and allow you to understand how we will allocate the marks. You should make use of the Fynesse framework (<https://github.com/lawrennd/fynesse_template>) for structuring your code.\n",
        "\n",
        "Remember the notebook you create should _tell a story_, any code that is not critical to that story can safely be placed into the associated analysis library and imported for use (structured as given in the Fynesse template)\n",
        "\n",
        "The maximum total mark for this assessment is 20. That mark is split into Three Questions below, each worth 5 marks each. Then a final 5 marks will be given for the quality, structure and reusability of the code and analysis you produce giving 20 marks in total.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb7e7c53",
      "metadata": {
        "id": "bb7e7c53"
      },
      "source": [
        "### Useful Links\n",
        "\n",
        "You may find some of the following links useful when building your system.\n",
        "\n",
        "University instuctions on Security and Privacy with AWS.\n",
        "\n",
        "https://help.uis.cam.ac.uk/service/network-services/hosting-services/AWS/aws-security-privacy\n",
        "\n",
        "Security Rules in AWS\n",
        "\n",
        "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.Scenarios.html#USER_VPC.Scenario4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d3ee6ce",
      "metadata": {
        "id": "9d3ee6ce"
      },
      "source": [
        "### Installing Your Library\n",
        "\n",
        "One artefact to be included in your submission is a python library structured according to the \"Access, Assess, Address\" standard for data science solutions. You will submit this library alongside your code. Use the cell below to perform the necessary installation instructions for your library.\n",
        "\n",
        "You should base your module on the template repository given by the Fynesse template repository. That should make it `pip` installable as below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13f71cb7",
      "metadata": {
        "id": "13f71cb7"
      },
      "outputs": [],
      "source": [
        "# Install your library here, if running on Colab\n",
        "%pip install git+https://github.com/andylolu2/ads-assignment.git"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2fafca0",
      "metadata": {
        "id": "c2fafca0"
      },
      "source": [
        "Your own library should be installed in the line above, then you can import it as usual (where you can either replace `fynesse` with the name you've given your analysis module or you can leave the name as `fynesse` as you prefer).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7701a5b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# active reload for rapid development\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db162b53",
      "metadata": {
        "id": "db162b53"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import osmnx\n",
        "import scipy.stats\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "\n",
        "from fynesse import access, assess, address, utils\n",
        "from fynesse.config import config\n",
        "\n",
        "plt.style.use(\"seaborn\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ead091ce",
      "metadata": {},
      "source": [
        "## Question 1. Accessing a Database of House Prices, Latitudes and Longitudes"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "26533cf6",
      "metadata": {
        "id": "26533cf6"
      },
      "source": [
        "The UK price paid data for housing in dates back to 1995 and contains millions of transactions. The size of the data makes it unwieldy to manipulate directly in python frameworks such as `pandas`. As a result we will host the data in a _relational database_.\n",
        "\n",
        "Using the following ideas.\n",
        "\n",
        "1. A cloud hosted database (such as MariaDB hosted on the AWS RDS service).\n",
        "2. The SQL language wrapped in appropriately structured python code.\n",
        "3. Joining of two databases.\n",
        "\n",
        "You will construct a database containing tables that contain all house prices, latitudes and longitudes from the UK house price data base since 1995.\n",
        "\n",
        "You will likely find the following resources helpful.\n",
        "\n",
        "1. Lecture 1, 2 and 3.\n",
        "2. Lab class 1 and 2.\n",
        "3. The UK Price Paid data for houses: <https://www.gov.uk/government/statistical-data-sets/price-paid-data-downloads>\n",
        "4. The UK ONS Data base of postcode latitude and longitudes: <https://www.getthedata.com/open-postcode-geo>\n",
        "\n",
        "Below we provide codeboxes and hints to help you develop your answer.\n",
        "\n",
        "_The main knowledge you need to do a first pass through this question will have been taught by the end of Lab Session 2 (11th November 2021). You will likely want to review your answer as part of **refactoring** your code and analysis pipeline shortly before hand in._\n",
        "\n",
        "_5 Marks_\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "fd689312",
      "metadata": {
        "id": "fd689312"
      },
      "source": [
        "### Task A"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0417492",
      "metadata": {},
      "source": [
        "Set up the database. You'll need to set up a database on AWS. You were guided in how to do this in the lab sessions. You should be able to use the same database instance you created in the lab, or you can delete that and start with a fresh instance. You'll remember from the lab that the database requires credentials (username, password) to access. It's good practice to store those credentials _outside_ the notebook so you don't accidentally share them by e.g. checking code into a repository.\n",
        "\n",
        "Call the database you use for this assessment `property_prices`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feaf09b9",
      "metadata": {
        "id": "feaf09b9"
      },
      "outputs": [],
      "source": [
        "# Set up credentials by adding \"db_username\", \"db_password\", \"db_host\", \"db_port\" to _config.yml.\n",
        "# Otherwise, set the values here by calling `set_credentials`.\n",
        "\n",
        "def set_credentials(username, password, host, port):\n",
        "    conf = config.config\n",
        "    conf[\"db_username\"] = username\n",
        "    conf[\"db_password\"] = password\n",
        "    conf[\"db_host\"] = host\n",
        "    conf[\"db_port\"] = port\n",
        "\n",
        "\n",
        "for k in [\"db_username\", \"db_password\", \"db_host\", \"db_port\"]:\n",
        "    assert k in config.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "036c419d",
      "metadata": {
        "id": "036c419d"
      },
      "outputs": [],
      "source": [
        "# Create database\n",
        "with access.create_connection() as conn:\n",
        "    with conn.cursor() as cursor:\n",
        "        cursor.execute(\"SET SQL_MODE = `NO_AUTO_VALUE_ON_ZERO`\")\n",
        "        cursor.execute(\"SET time_zone = `+00:00`\")\n",
        "        cursor.execute(\n",
        "            \"CREATE DATABASE IF NOT EXISTS `property_prices` DEFAULT CHARACTER SET utf8 COLLATE utf8_bin\"\n",
        "        )\n",
        "    conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73da5770",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confirm database exists\n",
        "access.sql_read(\"SHOW DATABASES\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a7b4219",
      "metadata": {},
      "source": [
        "### Task B"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "33c7237f",
      "metadata": {
        "id": "33c7237f"
      },
      "source": [
        "Create a database table called `pp_data` containing all the UK Price Paid data from the [gov.uk site](https://www.gov.uk/government/statistical-data-sets/price-paid-data-downloads). You'll need to prepare a new table to receive the data and upload the UK Price Paid data to your database instance. The total data is over 3 gigabytes in size. We suggest that rather than downloading the full data in CSV format, you use the fact that they have split the data into years and into different parts per year. For example, the first part of the data for 2018 is stored at <http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2018-part1.csv>. Each of these files is less than 100MB and can be downloaded very quickly to local disk, then uploaded using\n",
        "\n",
        "```\n",
        "LOCAL DATA LOAD INFILE 'filename' INTO TABLE `table_name`\n",
        "FIELDS TERMINATED BY ','\n",
        "LINES STARTING BY '' TERMINATED BY '\\n';\n",
        "```\n",
        "\n",
        "_Note_ this command should be wrapped and placed in an appropriately structured python module.\n",
        "\n",
        "Each 'data part' should be downloadable from the `gov.uk` site and uploadable to your database instance in a couple of seconds. By looping across the years and different parts, you should be able to robustly upload this large data set to your database instance in a matter of minutes.\n",
        "\n",
        "You may find the following schema useful in creation of your database:\n",
        "\n",
        "```\n",
        "--\n",
        "-- Table structure for table `pp_data`\n",
        "--\n",
        "DROP TABLE IF EXISTS `pp_data`;\n",
        "CREATE TABLE IF NOT EXISTS `pp_data` (\n",
        "  `transaction_unique_identifier` tinytext COLLATE utf8_bin NOT NULL,\n",
        "  `price` int(10) unsigned NOT NULL,\n",
        "  `date_of_transfer` date NOT NULL,\n",
        "  `postcode` varchar(8) COLLATE utf8_bin NOT NULL,\n",
        "  `property_type` varchar(1) COLLATE utf8_bin NOT NULL,\n",
        "  `new_build_flag` varchar(1) COLLATE utf8_bin NOT NULL,\n",
        "  `tenure_type` varchar(1) COLLATE utf8_bin NOT NULL,\n",
        "  `primary_addressable_object_name` tinytext COLLATE utf8_bin NOT NULL,\n",
        "  `secondary_addressable_object_name` tinytext COLLATE utf8_bin NOT NULL,\n",
        "  `street` tinytext COLLATE utf8_bin NOT NULL,\n",
        "  `locality` tinytext COLLATE utf8_bin NOT NULL,\n",
        "  `town_city` tinytext COLLATE utf8_bin NOT NULL,\n",
        "  `district` tinytext COLLATE utf8_bin NOT NULL,\n",
        "  `county` tinytext COLLATE utf8_bin NOT NULL,\n",
        "  `ppd_category_type` varchar(2) COLLATE utf8_bin NOT NULL,\n",
        "  `record_status` varchar(2) COLLATE utf8_bin NOT NULL,\n",
        "  `db_id` bigint(20) unsigned NOT NULL\n",
        ") DEFAULT CHARSET=utf8 COLLATE=utf8_bin AUTO_INCREMENT=1 ;\n",
        "```\n",
        "\n",
        "This schema is written by Dale Potter and can be found on Github here: <https://github.com/dalepotter/uk_property_price_data/blob/master/create_db.sql>\n",
        "\n",
        "You may also find it helpful to set up the following indexes in the database\n",
        "\n",
        "```\n",
        "--\n",
        "-- Indexes for table `pp_data`\n",
        "--\n",
        "ALTER TABLE `pp_data`\n",
        "ADD PRIMARY KEY (`db_id`);\n",
        "MODIFY `db_id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=1;\n",
        "CREATE INDEX `pp.postcode` USING HASH\n",
        "  ON `pp_data`\n",
        "    (postcode);\n",
        "CREATE INDEX `pp.date` USING HASH\n",
        "  ON `pp_data`\n",
        "    (date_of_transfer);\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e789b174",
      "metadata": {
        "id": "e789b174"
      },
      "source": [
        "In the box below, briefly describe what the schema is doing and why we will find it useful to create the indexes we have for the table we've created.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d7d9d674",
      "metadata": {
        "id": "d7d9d674"
      },
      "source": [
        "#### Answer\n",
        "\n",
        "The schema is unpacking the columns in the property prices csv data into suitable data types. \n",
        "\n",
        "The indexes are useful for speeding up lookup / joins on particular columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92da8c96",
      "metadata": {
        "id": "92da8c96"
      },
      "outputs": [],
      "source": [
        "# Create pp_data schema\n",
        "with access.create_connection(\"property_prices\") as conn:\n",
        "    with conn.cursor() as cursor:\n",
        "        cursor.execute(\n",
        "            \"\"\"\n",
        "                CREATE TABLE IF NOT EXISTS `pp_data` (\n",
        "                `transaction_unique_identifier` tinytext COLLATE utf8_bin NOT NULL,\n",
        "                `price` int(10) unsigned NOT NULL,\n",
        "                `date_of_transfer` date NOT NULL,\n",
        "                `postcode` varchar(8) COLLATE utf8_bin NOT NULL,\n",
        "                `property_type` varchar(1) COLLATE utf8_bin NOT NULL,\n",
        "                `new_build_flag` varchar(1) COLLATE utf8_bin NOT NULL,\n",
        "                `tenure_type` varchar(1) COLLATE utf8_bin NOT NULL,\n",
        "                `primary_addressable_object_name` tinytext COLLATE utf8_bin NOT NULL,\n",
        "                `secondary_addressable_object_name` tinytext COLLATE utf8_bin NOT NULL,\n",
        "                `street` tinytext COLLATE utf8_bin NOT NULL,\n",
        "                `locality` tinytext COLLATE utf8_bin NOT NULL,\n",
        "                `town_city` tinytext COLLATE utf8_bin NOT NULL,\n",
        "                `district` tinytext COLLATE utf8_bin NOT NULL,\n",
        "                `county` tinytext COLLATE utf8_bin NOT NULL,\n",
        "                `ppd_category_type` varchar(2) COLLATE utf8_bin NOT NULL,\n",
        "                `record_status` varchar(2) COLLATE utf8_bin NOT NULL,\n",
        "                `db_id` bigint(20) unsigned NOT NULL\n",
        "                ) DEFAULT CHARSET=utf8 COLLATE=utf8_bin AUTO_INCREMENT=1\n",
        "        \"\"\"\n",
        "        )\n",
        "        cursor.execute(\"ALTER TABLE `pp_data` ADD PRIMARY KEY (`db_id`)\")\n",
        "        cursor.execute(\n",
        "            \"ALTER TABLE `pp_data` MODIFY `db_id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=1\"\n",
        "        )\n",
        "    conn.commit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65cba23b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download and upload all data\n",
        "save_dir = Path(\"data\")\n",
        "part_url_format = config[\"pp_data_part_url_format\"]\n",
        "url_format = config[\"pp_data_url_format\"]\n",
        "\n",
        "urls = [url_format.format(year=2022)]\n",
        "paths = [save_dir / \"2022.csv\"]\n",
        "for year in range(1995, 2022):\n",
        "    for part in [1, 2]:\n",
        "        urls.append(part_url_format.format(year=year, part=part))\n",
        "        paths.append(save_dir / f\"{year}-{part}.csv\")\n",
        "\n",
        "# access.reset(table=\"pp_data\", database=\"property_prices\")\n",
        "\n",
        "for url, path in zip(urls, paths):\n",
        "    access.download(url, path)\n",
        "    access.upload(path, table=\"pp_data\", database=\"property_prices\", delimiter='\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c07804b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add index after uploading all data\n",
        "# Note: Creating index before uploading seems to make the upload process extremely slow\n",
        "with access.create_connection(\"property_prices\") as conn:\n",
        "    with conn.cursor() as cursor:\n",
        "        cursor.execute(\"CREATE INDEX `pp.postcode` USING HASH ON `pp_data` (postcode)\")\n",
        "        cursor.execute(\n",
        "            \"CREATE INDEX `pp.date` USING HASH ON `pp_data` (date_of_transfer)\"\n",
        "        )\n",
        "    conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eab1708",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify data is there\n",
        "access.sql_read(\"SELECT * FROM pp_data LIMIT 3\", \"property_prices\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b2be218d",
      "metadata": {},
      "source": [
        "### Task C"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "bb9ece66",
      "metadata": {
        "id": "bb9ece66"
      },
      "source": [
        "Create a database table called `postcode_data` containing the ONS Postcode information. <GetTheData.com> has organised data derived from the UK Office for National Statistics into a convenient CSV file. You can find details [here](https://www.getthedata.com/open-postcode-geo).\n",
        "\n",
        "The data you need can be found at this url: <https://www.getthedata.com/downloads/open_postcode_geo.csv.zip>. It will need to be unzipped before use.\n",
        "\n",
        "You may find the following schema useful for the postcode data (developed by Christian and Neil)\n",
        "\n",
        "```\n",
        "USE `property_prices`;\n",
        "--\n",
        "-- Table structure for table `postcode_data`\n",
        "--\n",
        "DROP TABLE IF EXISTS `postcode_data`;\n",
        "CREATE TABLE IF NOT EXISTS `postcode_data` (\n",
        "  `postcode` varchar(8) COLLATE utf8_bin NOT NULL,\n",
        "  `status` enum('live','terminated') NOT NULL,\n",
        "  `usertype` enum('small', 'large') NOT NULL,\n",
        "  `easting` int unsigned,\n",
        "  `northing` int unsigned,\n",
        "  `positional_quality_indicator` int NOT NULL,\n",
        "  `country` enum('England', 'Wales', 'Scotland', 'Northern Ireland', 'Channel Islands', 'Isle of Man') NOT NULL,\n",
        "  `lattitude` decimal(11,8) NOT NULL,\n",
        "  `longitude` decimal(10,8) NOT NULL,\n",
        "  `postcode_no_space` tinytext COLLATE utf8_bin NOT NULL,\n",
        "  `postcode_fixed_width_seven` varchar(7) COLLATE utf8_bin NOT NULL,\n",
        "  `postcode_fixed_width_eight` varchar(8) COLLATE utf8_bin NOT NULL,\n",
        "  `postcode_area` varchar(2) COLLATE utf8_bin NOT NULL,\n",
        "  `postcode_district` varchar(4) COLLATE utf8_bin NOT NULL,\n",
        "  `postcode_sector` varchar(6) COLLATE utf8_bin NOT NULL,\n",
        "  `outcode` varchar(4) COLLATE utf8_bin NOT NULL,\n",
        "  `incode` varchar(3)  COLLATE utf8_bin NOT NULL,\n",
        "  `db_id` bigint(20) unsigned NOT NULL\n",
        ") DEFAULT CHARSET=utf8 COLLATE=utf8_bin;\n",
        "```\n",
        "\n",
        "And again you'll want to set up indices for your table.\n",
        "\n",
        "```\n",
        "ALTER TABLE `postcode_data`\n",
        "ADD PRIMARY KEY (`db_id`);\n",
        "MODIFY `db_id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=1;\n",
        "CREATE INDEX `po.postcode` USING HASH\n",
        "  ON `postcode_data`\n",
        "    (postcode);\n",
        "```\n",
        "\n",
        "And you can load the CSV file into the table in one \"INFILE\".\n",
        "\n",
        "```\n",
        "LOAD DATA LOCAL INFILE 'open_postcode_geo.csv' INTO TABLE `postcode_data`\n",
        "FIELDS TERMINATED BY ','\n",
        "LINES STARTING BY '' TERMINATED BY '\\n';\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c968ce78",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create postcode_data schema\n",
        "with access.create_connection(\"property_prices\") as conn:\n",
        "    with conn.cursor() as cursor:\n",
        "        cursor.execute(\n",
        "            \"\"\"\n",
        "                CREATE TABLE IF NOT EXISTS `postcode_data` (\n",
        "                `postcode` varchar(8) COLLATE utf8_bin NOT NULL,\n",
        "                `status` enum('live','terminated') NOT NULL,\n",
        "                `usertype` enum('small', 'large') NOT NULL,\n",
        "                `easting` int unsigned,\n",
        "                `northing` int unsigned,\n",
        "                `positional_quality_indicator` int NOT NULL,\n",
        "                `country` enum('England', 'Wales', 'Scotland', 'Northern Ireland', 'Channel Islands', 'Isle of Man') NOT NULL,\n",
        "                `latitude` decimal(11,8) NOT NULL,\n",
        "                `longitude` decimal(10,8) NOT NULL,\n",
        "                `postcode_no_space` tinytext COLLATE utf8_bin NOT NULL,\n",
        "                `postcode_fixed_width_seven` varchar(7) COLLATE utf8_bin NOT NULL,\n",
        "                `postcode_fixed_width_eight` varchar(8) COLLATE utf8_bin NOT NULL,\n",
        "                `postcode_area` varchar(2) COLLATE utf8_bin NOT NULL,\n",
        "                `postcode_district` varchar(4) COLLATE utf8_bin NOT NULL,\n",
        "                `postcode_sector` varchar(6) COLLATE utf8_bin NOT NULL,\n",
        "                `outcode` varchar(4) COLLATE utf8_bin NOT NULL,\n",
        "                `incode` varchar(3)  COLLATE utf8_bin NOT NULL,\n",
        "                `db_id` bigint(20) unsigned NOT NULL\n",
        "                ) DEFAULT CHARSET=utf8 COLLATE=utf8_bin;\n",
        "        \"\"\"\n",
        "        )\n",
        "        cursor.execute(\"ALTER TABLE `postcode_data` ADD PRIMARY KEY (`db_id`)\")\n",
        "        cursor.execute(\n",
        "            \"ALTER TABLE `postcode_data` MODIFY `db_id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=1\"\n",
        "        )\n",
        "    conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ee86ecc",
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p ./data/postcode\n",
        "!wget -P ./data/postcode https://www.getthedata.com/downloads/open_postcode_geo.csv.zip\n",
        "!unzip ./data/postcode/open_postcode_geo.csv.zip -d ./data/postcode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bbcc563",
      "metadata": {},
      "outputs": [],
      "source": [
        "access.upload(\n",
        "    Path(\"data\") / \"postcode\" / \"open_postcode_geo.csv\", \n",
        "    table=\"postcode_data\", \n",
        "    database=\"property_prices\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7114ea4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "with access.create_connection(\"property_prices\") as conn:\n",
        "    with conn.cursor() as cursor:\n",
        "        cursor.execute(\"CREATE INDEX `po.postcode` USING HASH ON `postcode_data` (postcode)\")\n",
        "    conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87902ce0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# verify data has been created\n",
        "access.sql_read(\"SELECT * FROM postcode_data LIMIT 5\", \"property_prices\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "350a3fb3",
      "metadata": {},
      "source": [
        "### Task D"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d67ca4ef",
      "metadata": {
        "id": "d67ca4ef"
      },
      "source": [
        "These data can now be joined to form a new table that contains house price paid and latitude longitude of the house. We could create a new table that contains all this information. However, the computation of that table will take some time because of the size of the two existing tables in the join.\n",
        "\n",
        "Instead, we're going to exploit the nature of the task. To build our prediction model, we're going to use the prices for a particular region in a given time period. This means we can select that region and time period and build the joined data only from the relevent rows from the two tables. This will save time on the join.\n",
        "\n",
        "Whether this is a good idea or not in a live system will depend on how often these predictions are required. If it's very often, it would likely be better to store the entired database joined, because the one-off cost for that join is amortised across all the future predictions. If only a few predictions are required (like in our lab class) then doing that join on the fly might be better. In that case you can make use of an _inner join_ for this data set creation.\n",
        "\n",
        "```\n",
        "USE `property_prices`;\n",
        "--\n",
        "-- Table structure for table `prices_coordinates_data`\n",
        "--\n",
        "DROP TABLE IF EXISTS `prices_coordinates_data`;\n",
        "CREATE TABLE IF NOT EXISTS `prices_coordinates_data` (\n",
        "  `price` int(10) unsigned NOT NULL,\n",
        "  `date_of_transfer` date NOT NULL,\n",
        "  `postcode` varchar(8) COLLATE utf8_bin NOT NULL,\n",
        "  `property_type` varchar(1) COLLATE utf8_bin NOT NULL,\n",
        "  `new_build_flag` varchar(1) COLLATE utf8_bin NOT NULL,\n",
        "  `tenure_type` varchar(1) COLLATE utf8_bin NOT NULL,\n",
        "  `locality` tinytext COLLATE utf8_bin NOT NULL,\n",
        "  `town_city` tinytext COLLATE utf8_bin NOT NULL,\n",
        "  `district` tinytext COLLATE utf8_bin NOT NULL,\n",
        "  `county` tinytext COLLATE utf8_bin NOT NULL,\n",
        "  `country` enum('England', 'Wales', 'Scotland', 'Northern Ireland', 'Channel Islands', 'Isle of Man') NOT NULL,\n",
        "  `lattitude` decimal(11,8) NOT NULL,\n",
        "  `longitude` decimal(10,8) NOT NULL,\n",
        "  `db_id` bigint(20) unsigned NOT NULL\n",
        ") DEFAULT CHARSET=utf8 COLLATE=utf8_bin AUTO_INCREMENT=1 ;\n",
        "\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "336ee680",
      "metadata": {},
      "outputs": [],
      "source": [
        "# method implemented in \"query_price\" function in library\n",
        "lat, long = config[\"sample_coords\"][\"christs\"]\n",
        "\n",
        "assess.query_price(\n",
        "    date_min=\"2018-02-29\", \n",
        "    date_max=\"2019-02-29\", \n",
        "    bbox=utils.bbox(lat, long, 500, 500), \n",
        "    limit=3,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "349da89c",
      "metadata": {},
      "source": [
        "## Question 2. Accessing OpenStreetMap and Assessing the Available Features"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "df36e5d1",
      "metadata": {
        "id": "df36e5d1"
      },
      "source": [
        "In question 3 you will be given the task of constructing a prediction system for property price levels at a given location. We expect that knowledge of the local region around the property should be helpful in making those price predictions. To evaluate this we will now look at [OpenStreetMap](https://www.openstreetmap.org) as a data source.\n",
        "\n",
        "The tasks below will guide you in accessing and assessing the OpenStreetMap data. The code you write will eventually be assimilated in your python module, but documentation of what you've included and why should remain in the notebook below.\n",
        "\n",
        "Accessing OpenStreetMap through its API can be done using the python library `osmx`. Using what you have learned about the `osmx` interface in the lectures, write general code for downloading points of interest and other relevant information that you believe may be useful for predicting house prices. Remembering the perspectives we've taken on _data science as debugging_, the remarks we've made when discussing _the data crisis_ of the importance of reusability in data analysis, and the techniques we've explored in the labsessions for visualising features and exploring their correlation use the notebook to document your assessment of the OpenStreetMap data as a potential source of data.\n",
        "\n",
        "The knowledge you need to do a first pass through this question will have been taught by end of lab session three (16th November 2021). You will likely want to review your answer as part of _refactoring_ your code and analysis pipeline shortly before hand in.\n",
        "\n",
        "You should write reusable code that allows you to explore the characteristics of different points of interest. Looking ahead to question 3 you'll want to incorporate these points of interest in your prediction code.\n",
        "\n",
        "_5 marks_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36efc4a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "dist = 500  # in meters\n",
        "\n",
        "lat, long = config[\"sample_coords\"][\"christs\"]\n",
        "bbox1 = utils.bbox(lat, long, dist, dist)\n",
        "\n",
        "lat, long = config[\"sample_coords\"][\"hammersmith\"]\n",
        "bbox2 = utils.bbox(lat, long, dist, dist)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "70d160a1",
      "metadata": {},
      "source": [
        "#### Access OpenStreetMap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "042a2863",
      "metadata": {
        "id": "042a2863"
      },
      "outputs": [],
      "source": [
        "# Helper function to download points of interest from OpenStreetMap\n",
        "# Note: has been incorporated in the library but included here from completeness\n",
        "def pois_from_bbox(bbox, tags):\n",
        "    gdf = osmnx.geometries_from_bbox(*bbox, tags=tags)\n",
        "    gdf = gdf.to_crs(crs=3857)\n",
        "    gdf[\"geometry_area\"] = gdf.area\n",
        "    gdf = gdf.to_crs(crs=4326)\n",
        "    return gdf"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7bababb5",
      "metadata": {},
      "source": [
        "##### Tags\n",
        "\n",
        "- Some manual trial and error is done test out different tags to download from OpenStreetMap. \n",
        "- The more detailed description about each tag (90K+!) in OpenStreetMap can be found on https://wiki.openstreetmap.org/wiki/Map_features.\n",
        "- It was not possible to explore every tag but the most popular housing-related tags has been included.\n",
        "- It is possible that there are some more useful tag out there which could be insightful. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cda25ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "tags = {\n",
        "    \"building\": True, \n",
        "    \"residential\": True, \n",
        "    \"shop\": True, \n",
        "    \"natural\": True, \n",
        "    \"amenity\": True, \n",
        "    \"leisure\": True\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd76170e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example\n",
        "pois_from_bbox(bbox1, tags).head(2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4b4f7e87",
      "metadata": {},
      "source": [
        "#### Assess features from OpenStreetMap"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0462b2c5",
      "metadata": {},
      "source": [
        "#### Data types and dealing with missing features\n",
        "\n",
        "Q: What are the data types of each column? How to appropriately fill in the missing features?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee3036c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This function has been incorporated into the library\n",
        "def clean_data(*bboxs):\n",
        "    # built-in conversion seems to work well, and using <NA> to handle missing values seems reasonable\n",
        "    gdf = pd.concat([pois_from_bbox(bbox, tags) for bbox in bboxs])\n",
        "    return gdf.convert_dtypes()\n",
        "\n",
        "gdf = clean_data(bbox1, bbox2)\n",
        "dtypes = gdf.dtypes.astype(str).to_frame(\"dtype\").reset_index()\n",
        "print(dtypes.groupby(\"dtype\")[\"index\"].agg(list))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "fa1e0317",
      "metadata": {},
      "source": [
        "Key observations:\n",
        "- Most data from OpenStreetMap are given as strings, except for \"special\" columns listed above."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0143fb67",
      "metadata": {},
      "source": [
        "#### Sparsity of features\n",
        "\n",
        "Q: How much of the OpenStreetMap data is actually just null values? What features carry the least number of null values?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "969e6f60",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-display\n",
        "\n",
        "def sparsity():\n",
        "    gdf = clean_data(bbox1, bbox2)\n",
        "    feature_sparsity = gdf.notna().mean().sort_values(ascending=False)\n",
        "    return feature_sparsity\n",
        "\n",
        "n = 25\n",
        "feature_sparsity = sparsity()\n",
        "fig = plt.figure(figsize=(12, 4))\n",
        "\n",
        "ax = fig.add_subplot(1, 3, (1, 2))\n",
        "ax = feature_sparsity.iloc[:n].plot(kind=\"bar\", width=0.8, ax=ax)\n",
        "ax.set_xlabel(f\"Top {n} OSM features\")\n",
        "ax.set_ylabel(\"Proportion of non-null entries\")\n",
        "\n",
        "ax = fig.add_subplot(1, 3, 3)\n",
        "ax = feature_sparsity.plot(kind=\"hist\", bins=20, edgecolor=\"w\")\n",
        "ax.set_yscale(\"log\")\n",
        "ax.set_xlabel(\"Proportion of non-null entries\")\n",
        "ax.set_ylabel(\"Number of features\")\n",
        "\n",
        "fig.tight_layout()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "50ecf529",
      "metadata": {},
      "source": [
        "Key observations\n",
        "- Most features do not occur in more than 5% of the rows. (They are usually nulls).\n",
        "- Less than 20 features contain more than 20% of non-null rows. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2716efb9",
      "metadata": {},
      "source": [
        "#### Cardinality of the features\n",
        "\n",
        "Q: What is the cardinality of each feature? Is a feature heavily unique or categorical?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "010303dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-display\n",
        "def top_features(n: int):\n",
        "    gdf = clean_data(bbox1, bbox2)\n",
        "    gdf = gdf.drop(columns=[\"nodes\", \"ways\"])\n",
        "\n",
        "    num_unique = gdf.nunique().to_frame(\"num_unique\")\n",
        "    num_not_na = gdf.notna().sum().to_frame(\"num_not_na\")\n",
        "    dtype = gdf.dtypes.to_frame(\"dtype\")\n",
        "    dtype[\"dtype\"][gdf.applymap(type).eq(str).any()] = pd.StringDtype()\n",
        "\n",
        "    features = pd.merge(num_unique, num_not_na, right_index=True, left_index=True)\n",
        "    features = pd.merge(features, dtype, right_index=True, left_index=True)\n",
        "    features[\"uniqueness\"] = features[\"num_unique\"] / features[\"num_not_na\"]\n",
        "    features = features.sort_values(\"num_not_na\", ascending=False)\n",
        "\n",
        "    return features[:n]\n",
        "\n",
        "fig, [ax1, ax2, ax3] = plt.subplots(3, 1, sharex=True, figsize=(8, 6))\n",
        "\n",
        "n = 25\n",
        "top_n = top_features(n)\n",
        "top_n[\"uniqueness\"].plot(kind=\"bar\", width=0.9, ax=ax1)\n",
        "ax1.set_ylabel(\"Prop. unique entries\")\n",
        "\n",
        "top_n[\"num_unique\"].plot(kind=\"bar\", width=0.9, ax=ax2)\n",
        "ax2.set_ylabel(\"No. unique entries\")\n",
        "ax2.set_yscale(\"log\")\n",
        "\n",
        "top_n[\"num_not_na\"].plot(kind=\"bar\", width=0.9, ax=ax3)\n",
        "ax3.set_xlabel(f\"Top {n} OSM features\")\n",
        "ax3.set_ylabel(\"No. non-na entries\")\n",
        "\n",
        "fig.tight_layout()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5b8ced60",
      "metadata": {},
      "source": [
        "Key observations / important notes:\n",
        "- Roughly half of the top 20 OSM features are not categorical. (e.g. `name`, `brand`, `website`)\n",
        "- This is **not** representative of the whole OSM dataset. As we are only analysing a subset of the OSM dataset, \n",
        "    some non-categorical features might appear as categorical (e.g. `addr: city`).\n",
        "- Features that might be suitable to be used as categorical features include `addr:city`, `addr:street`, `building`, `amenity`, `source`, `shop`, `operator`, `building:levels`, `natural`. They are identified by having a low proportion of unique values (first plot)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2216d9cc",
      "metadata": {},
      "source": [
        "#### Feature occurrence correlation\n",
        "\n",
        "Q: What features are usually present together? What features don't usually appear together?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa5c2051",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-display\n",
        "def co_occurrence(n):\n",
        "    top_f = top_features(n).index\n",
        "    corr = gdf[top_f].notna().corr()\n",
        "    # drop columns and rows related to geometry\n",
        "    corr = corr.dropna(axis=1, how=\"all\").dropna(axis=0, how=\"all\")\n",
        "    # sort the labels alphabetically for better clarity\n",
        "    corr = corr.sort_index().reindex(sorted(corr.columns), axis=1)\n",
        "    return corr\n",
        "\n",
        "n = 20\n",
        "corr = co_occurrence(n)\n",
        "ax = sns.heatmap(corr, cmap=\"RdBu\", center=0)\n",
        "_ = ax.set_title(\"Correlation in occurrence of top 20 OSM features\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5c7f5fbe",
      "metadata": {},
      "source": [
        "Key observations:\n",
        "- The correlation in occurrence is generally positive among all features. \n",
        "- There are these hierarchical features (e.g. `brand`, `brand:wikidata`, `brand:wikipedia`) that demonstrate really high correlation.\n",
        "- Some features appear to be negatively correlated, suggesting conflicting / exclusiveness of some features. For example, `building` and `amenity`."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cf8a46b5",
      "metadata": {},
      "source": [
        "#### Visualising the features\n",
        "\n",
        "Q: How does the features look like on a map? How diverse / dense are they? How much data is there?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b464caf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9 features of inspect. Manually chose \"interesting\" features \n",
        "# out of the 25 most prominent ones listed in the bar plot above.\n",
        "\n",
        "features = [\n",
        "    \"building\", \"geometry_area\", \"leisure\", \"natural\", \"amenity\", \n",
        "    \"brand\", \"shop\", \"operator\", \"building:levels\", \n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9a95f13",
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualise(bbox, feature, legend, ax):\n",
        "    gdf = clean_data(bbox)\n",
        "    graph = osmnx.graph_from_bbox(*bbox)\n",
        "    nodes, edges = osmnx.graph_to_gdfs(graph)\n",
        "    edges.plot(ax=ax, linewidth=1, edgecolor=\"dimgray\")\n",
        "    \n",
        "    is_num = pd.api.types.is_numeric_dtype(gdf[feature].dtype)\n",
        "    if is_num:\n",
        "        gdf[feature] = gdf[feature].astype(float)\n",
        "\n",
        "    gdf.plot(\n",
        "        ax=ax, \n",
        "        column=feature, \n",
        "        categorical=not is_num, \n",
        "        cmap=\"rainbow\",\n",
        "        alpha=0.9, \n",
        "        markersize=10, \n",
        "        edgecolor=\"b\", \n",
        "        linewidth=0.5,\n",
        "        legend=legend,\n",
        "        missing_kwds={\n",
        "            \"color\": \"silver\",\n",
        "            \"alpha\": 0.4,\n",
        "            \"markersize\": 10,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    ax.set_xlim(bbox[3], bbox[2])\n",
        "    ax.set_ylim(bbox[1], bbox[0])\n",
        "    ax.set_xlabel(\"Longitude\")\n",
        "    ax.set_ylabel(\"Latitude\")\n",
        "    ax.set_title(feature)\n",
        "\n",
        "def bbox_features(bbox):\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "    for i, feature in enumerate(features):\n",
        "        ax = fig.add_subplot(3, 3, i + 1)\n",
        "        visualise(bbox, feature, False, ax)\n",
        "\n",
        "    fig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d730072",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-display\n",
        "with plt.style.context(\"default\"):\n",
        "    bbox_features(bbox1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9e3088d",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-display\n",
        "with plt.style.context(\"default\"):\n",
        "    bbox_features(bbox2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5a9f0223",
      "metadata": {},
      "source": [
        "Key observations\n",
        "- Features have roughly a 50 / 50 split between \"ways\" (areas) and nodes. \n",
        "- Most features are indeed missing for most ways / nodes. \n",
        "- The selected tags don't have any features for roads."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f4435319",
      "metadata": {},
      "source": [
        "## Question 3. Addressing a Property Price Prediction Question"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "09a0e365",
      "metadata": {
        "id": "09a0e365"
      },
      "source": [
        "For your final tick, we will be asking you to make house price predictions for a given location, date and property type in the UK. You will provide a function that takes input a latitude and longitude as well as the `property_type` (either type\" of property (either `F` - flat, `S` - semidetached, `D` - detached, `T` - terraced or `O` other). Create this function in the `address.py` file, for example in the form,\n",
        "\n",
        "```\n",
        "def predict_price(latitude, longitude, date, property_type):\n",
        "    \"\"\"Price prediction for UK housing.\"\"\"\n",
        "    pass\n",
        "```\n",
        "\n",
        "We suggest that you use the following approach when building your prediction.\n",
        "\n",
        "1. Select a bounding box around the housing location in latitude and longitude.\n",
        "2. Select a data range around the prediction date.\n",
        "3. Use the data ecosystem you have build above to build a training set from the relevant time period and location in the UK. Include appropriate features from OSM to improve the prediction.\n",
        "4. Train a linear model on the data set you have created.\n",
        "5. Validate the quality of the model.\n",
        "6. Provide a prediction of the price from the model, warning appropriately if your validation indicates the quality of the model is poor.\n",
        "\n",
        "The knowledge you need to do a first pass through this question will have been taught by end of lab session four (25th November 2021). You will likely want to review your answer as part of _refactoring_ your code shortly before hand in.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f3d0036d",
      "metadata": {},
      "source": [
        "### Discovering possible features for house price prediction"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "016b12e8",
      "metadata": {},
      "source": [
        "#### Distribution of prices\n",
        "\n",
        "Q: How are the prices distributed overall? It is normal, or something else? Answering this question can give hints on what to choose as the link function / transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a945dd8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "lat, long = config[\"sample_coords\"][\"christs\"]\n",
        "christs_bbox = utils.bbox(lat, long, 2000, 2000)\n",
        "\n",
        "lat, long = config[\"sample_coords\"][\"hammersmith\"]\n",
        "hsm_bbox = utils.bbox(lat, long, 2000, 2000)\n",
        "\n",
        "christs_df = assess.query_price(bbox=christs_bbox)\n",
        "hsm_df = assess.query_price(bbox=hsm_bbox)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "387f2f1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot(df, ax, name):\n",
        "    prices = np.log10(df[\"price\"])\n",
        "    mean = np.mean(prices)\n",
        "    std = np.std(prices)\n",
        "    ax.hist(prices, bins=50, density=True)\n",
        "\n",
        "    x = np.linspace(prices.min(), prices.max(), 1000)\n",
        "    p = scipy.stats.norm.pdf(x, mean, std)\n",
        "    ax.plot(x, p, \"k\", lw=1.5, alpha=0.8)\n",
        "    ax.set_title(name)\n",
        "    ax.set_ylabel(\"Probability\")\n",
        "\n",
        "fig, [ax1, ax2] = plt.subplots(2, 1, figsize=(8, 5), sharex=True)\n",
        "\n",
        "plot(christs_df, ax1, \"Christ's\")\n",
        "plot(hsm_df, ax2, \"Hammersmith\")\n",
        "ax2.set_xlabel(\"Log (base 10) price\")\n",
        "\n",
        "fig.tight_layout()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "92fa9dfb",
      "metadata": {},
      "source": [
        "Key observations:\n",
        "- The prices has a log-normal distribution overall. This suggests a logarithmic link function / transform is suitable for the task."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a30b498a",
      "metadata": {},
      "source": [
        "#### Features in the property prices dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1b899bcf",
      "metadata": {},
      "source": [
        "##### Time vs price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff1c9ee0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_date(df, name, *args, **kwargs):\n",
        "    t = df[\"date_of_transfer\"]\n",
        "    grouped = df.groupby(by=[t.dt.year, t.dt.quarter])[\"price\"]\n",
        "\n",
        "    # using median instead of mean for 2 reasons: \n",
        "    # 1. median is directly comparable to the upper / lower quantile\n",
        "    # 2. when testing, the mean seems to be heavily influenced by some extreme values\n",
        "    price = grouped.median()\n",
        "    up = grouped.quantile(0.75)\n",
        "    low = grouped.quantile(0.25)\n",
        "\n",
        "    years = price.index.get_level_values(0).to_series().reset_index(drop=True).astype(str)\n",
        "    quarter = price.index.get_level_values(1).to_series().reset_index(drop=True)\n",
        "    month = (quarter * 3).astype(str).str.zfill(2)\n",
        "    x = pd.to_datetime(years + month, format=\"%Y%m\")\n",
        "\n",
        "    plt.plot(x, price, label=name, ls=\"--\", *args, **kwargs)\n",
        "    plt.plot(x, low, *args, **kwargs)\n",
        "    plt.plot(x, up, *args, **kwargs)\n",
        "    plt.fill_between(x, low, up, alpha=0.3, *args, **kwargs)\n",
        "\n",
        "plot_date(hsm_df, \"Hammersmith\", color=\"red\", lw=1)\n",
        "plot_date(christs_df, \"Chirst's\", color=\"cyan\", lw=1)\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.title(\"Average price per month over time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Average price\")\n",
        "plt.tight_layout()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cb928b2f",
      "metadata": {},
      "source": [
        "Key observations:\n",
        "- There is somewhat linear relationship between the price and time. \n",
        "- The noise / uncaptured variation seems to increase as the price increases. This suggests a log transformation of the price might be more suitable than a logarithmic link function."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b1b9e3d5",
      "metadata": {},
      "source": [
        "##### Property type vs price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2fda149",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.concat([christs_df.assign(name=\"Christ's\"), hsm_df.assign(name=\"Hammersmith\")])\n",
        "df[\"price\"] = np.log10(df[\"price\"].astype(float))\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
        "for i, feature in enumerate([\"property_type\", \"new_build_flag\", \"tenure_type\"]):\n",
        "    sns.violinplot(\n",
        "        data=df, x=feature, y=\"price\", hue=\"name\", scale=\"count\", cut=0,\n",
        "        linewidth=1, split=True, ax=axs[i]\n",
        "    )\n",
        "    axs[i].legend().remove()\n",
        "    axs[i].set(ylabel=None, xlabel=feature.replace(\"_\", \" \").capitalize())\n",
        "\n",
        "axs[0].set_ylabel(\"Log (base 10) price\")\n",
        "axs[-1].legend(title=\"Name\")\n",
        "plt.tight_layout()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "33a99be8",
      "metadata": {},
      "source": [
        "Key observations:\n",
        "- The \"other\" property type stands out from the rest as it has a much higher variation. No significant difference in the price distribution between the other property types.\n",
        "- No significant relationship between new build flag and the price, maybe an overall slightly higher price for newly built properties.\n",
        "- Overall higher price for `F` (Freehold) tenures. \n",
        "- Unfortunately, there is no obvious way to fill in `new_build_flag` or `tenure_type` when it is not given, so we cannot easily make use of any correlation between them and the price."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cf722f33",
      "metadata": {},
      "source": [
        "##### Other features in property prices dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5c558fb6",
      "metadata": {},
      "source": [
        "The other features in the property prices dataset are not likely to be useful due to various reasons / assumptions:\n",
        "\n",
        "- `postcode`: Unique to each property.\n",
        "- `locality`: Variance in price due to location can already be captured by only training on a data set sampled from the local area.\n",
        "- `town_city`: Same as above.\n",
        "- `district`: Same as above.\n",
        "- `county`: Same as above.\n",
        "- `country`: Same as above.\n",
        "- `latitude`: Same as above.\n",
        "- `longitude`: Same a above."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "983142ea",
      "metadata": {},
      "source": [
        "#### Features in OpenStreetMap"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "81e01400",
      "metadata": {},
      "source": [
        "The first problem that needs to be addressed is how to connect the OpenStreetMap data with the property prices. Here are some ideas:\n",
        "1. Associate an entry in the property price data set with a specific building (a `way` element) from OpenStreetMap.\n",
        "   - Assumptions:\n",
        "     - The location in both data sets are precise and align with each other.\n",
        "     - All properties exists as a building in OpenStreetMap. \n",
        "   - Pros:\n",
        "     - The features from OpenStreetMap specific to a building of interest should have a strong correlation with its price.\n",
        "   - Cons:\n",
        "     - At inference time, the location provided must be contained in a building from OpenStreetMap. Otherwise, there needs to be a way to handle such missing values.\n",
        "2. Associate an entry in the property price data set with features from OpenStreetMap within some distance.\n",
        "   - Assumptions:\n",
        "     - The price of a property is largely determined by the area that it is in.\n",
        "   - Pros:\n",
        "     - Does not require the location provided at inference time to always relate to a house.\n",
        "     - Can possibly make use of more data from OpenStreetMap as many will be aggregated and be associated to a property's price.\n",
        "   - Cons:\n",
        "     - Increases some \"fussiness\" in the features which inherently limits how accurate the model can get due to un-specificity of the features.\n",
        "\n",
        "Importantly, both methods assume that the true OpenStreetMap data is **static over time**. For specific locations / times, this might not be a good assumption. For example, the area around a house in 1995 is likely to be different from the area now (which OpenStreetMap depicts).\n",
        "\n",
        "A better approach might be a combination of both ideas: Aggregate features within some distance to a property, but weight those features by their proximity to the property itself."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e5e4309a",
      "metadata": {},
      "source": [
        "##### Selecting features from OpenStreetMap"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2d6ccd4a",
      "metadata": {},
      "source": [
        "Due to the enormous amounts of features from OpenStreetMap, it is not practical to thoroughly inspect all of them. Instead, we will select some of the most prominent features from OpenStreetMap manually by a combination of the analysis from question 2 and the modeller's inductive bias on what features are most likely going to be correlated with the price.\n",
        "The set of features that will be considered is `building`, `amenity`, `capacity`, `building:levels`, `shop`, `natural`, and `geometry`.\n",
        "\n",
        "We will inspect such selected features in detail:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ee4305d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example location to inspect\n",
        "radius = 200\n",
        "pp_gdf = address.expanded_pp_data(christs_bbox, datetime(2020, 1, 1), datetime(2022, 1, 1), radius)\n",
        "osm_gdf = address.filtered_osm_data(christs_bbox, set(config[\"osm_features\"].keys()))\n",
        "gdf = utils.spatial_join(pp_gdf, osm_gdf, how=\"left\", lsuffix=\"pp\", rsuffix=\"osm\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "12fb5ddd",
      "metadata": {},
      "source": [
        "Data types of selected features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74db446f",
      "metadata": {},
      "outputs": [],
      "source": [
        "for feature in config[\"osm_features\"]:\n",
        "    values = gdf[feature].unique()\n",
        "    dtypes = {utils.get_type(v).__name__ for v in values}\n",
        "    print(f\"Dtypes of {feature}: {dtypes}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "185a4367",
      "metadata": {},
      "source": [
        "From the above, we see that there are numeric types in `capacity` and `building:levels`. This suggests we might be able to process them as numbers instead of a categorical variable. \n",
        "\n",
        "Interestingly, `capacity` also has entries of type `str`. Let's inspect what those values are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdb3210f",
      "metadata": {},
      "outputs": [],
      "source": [
        "str_values = {v for v in gdf[\"capacity\"].unique() if utils.get_type(v) == str}\n",
        "str_values"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c8399771",
      "metadata": {},
      "source": [
        "Given such ambiguous embeddings, a reasonable way to handle such entries is to treat them as nulls.\n",
        "\n",
        "The next question is: How do we handle null values?\n",
        "- For categorical features (`building`, `amenity`, `shop`, `natural`), these can be handled easily by using a zero vector in their one-hot encoded form, representing that non of the features are present. \n",
        "- For numerical features (`capacity`, `building:levels`), a reasonable way is to fill missing entries by the mean / median value in the training set. A design choice is made to use the median as it is less susceptible to extreme values. This is implemented in `address.encode_numerical(features, fill_value=None)`.\n",
        "\n",
        "Handling categorical features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4b03756",
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_features = [\"building\", \"amenity\", \"shop\", \"natural\"]\n",
        "fig, axs = plt.subplots(len(categorical_features), 1, figsize=(8, 8))\n",
        "for i, feature in enumerate(categorical_features):\n",
        "    values = gdf[feature].value_counts()\n",
        "    axs[i].bar(values.index, values)\n",
        "    axs[i].set(xticklabels=[], title=feature.capitalize())\n",
        "fig.tight_layout()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3204382b",
      "metadata": {},
      "source": [
        "From the above, we can see that the categorical features exhibit a typical \"long-tailed\" distribution. It will be unwise to include categories that are part of the long tail as they only appear in small number of samples and so are not likely to be informative at inference time. Using such features could also make the model susceptible to over-fitting. \n",
        "\n",
        "To handle this, a reasonable approach is to select some cutoff point and group all categories in the tail into an artificial \"\\<OTHER\\>\" category. This is implemented in `address.encode_categorical(features, cutoff, categories)`.\n",
        "\n",
        "The overall encoding of OSM features is handled in `address.encode_feature(gdf, categorical_cutoff, eval_mapping)`.\n",
        "\n",
        "Now that we have processed all the features from OpenStreetMap, we need to aggregate such data associated with each property price. As discussed above, we will do so by weighting the data from OpenStreetMap by how close it is to the property location. This is implemented in `address.agg_features(df)`."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c69b4592",
      "metadata": {},
      "source": [
        "Other functions in the library:\n",
        "- `address.expanded_pp_data(bbox, date_min, date_max, radius)` returns the property prices data in the certain area (in bbox) and within some time range, with each point expanded to be an circle with `radius`. This acts as the \"receptive field\" for joining with OpenStreetMap data afterwards.\n",
        "- `address.sample_pp_data(latitude, longitude, date, property_type, radius)` returns a sample property price entry constructed by the values of the arguments. This is used for creating a point for inference.\n",
        "- `address.filtered_osm_data(bbox, features)` returns the relevant subset of features from OpenStreetMap."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1a6aea96",
      "metadata": {},
      "source": [
        "### Predicting the price"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2660b9bf",
      "metadata": {},
      "source": [
        "We will use the features constructed from the above to predict the price. As discussed above, predicting the log-transform of the price seems to be a suitable modelling choice. A simple model is to use an OLS model. To whole procedure consists of the following steps:\n",
        "1. Download the relevant property prices and OpenStreetMap data. \n",
        "2. Split the property prices data set into \"train\" and \"test.\n",
        "3. Process features (which depends on the data seen) and join the train property prices with OpenStreetMap data. \n",
        "4. Apply **exactly** the same processing to the test set and join it with OpenStreetMap data.\n",
        "5. Fit an OLS model on the training set.\n",
        "6. Evaluate on the test set. Raise a warning if the error (mse) is much larger compared to that on the training set, suggesting over-fitting.\n",
        "7. Construct the prediction input and retrieve a prediction of the price.\n",
        "\n",
        "The above is implemented in `address.predict_price`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d011912",
      "metadata": {},
      "outputs": [],
      "source": [
        "lat, long = config[\"sample_coords\"][\"christs\"]\n",
        "results, train_x, train_y, test_x, test_y, pred_x, pred_y = address.predict_price(\n",
        "    lat, long, datetime(2015, 12, 1), \"T\", \n",
        "    t_days=365*5, bbox_length=2000, radius=200, categorical_cutoff=0.85\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dcfdbc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_results(results, x, y, ax):\n",
        "    y_pred_mean, y_pred_low, y_pred_up = utils.predict(results, x)\n",
        "\n",
        "    y_err = np.concatenate([\n",
        "        [y_pred_mean - y_pred_low], [y_pred_up - y_pred_mean]\n",
        "    ])\n",
        "    ax.errorbar(np.log10(y), y_pred_mean, yerr=y_err, fmt=\"none\", elinewidth=1)\n",
        "    ax.scatter(np.log10(y), y_pred_mean, s=5, alpha=0.8)\n",
        "\n",
        "    lo1, hi1 = ax.get_xlim()\n",
        "    lo2, hi2 = ax.get_ylim()\n",
        "    p1 = max(lo1, lo2)\n",
        "    p2 = min(hi1, hi2)\n",
        "    ax.plot([p1, p2], [p1, p2], alpha=0.85,\n",
        "        color=\"k\", linewidth=1, label=\"Line of optimal model\"\n",
        "    )\n",
        "    ax.set_xlabel(\"Log (base 10) true price\")\n",
        "    ax.set_ylabel(\"Log (base 10) predicted price\")\n",
        "    ax.legend()\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), sharey=True, sharex=True)\n",
        "\n",
        "ax1.set_title(\"Train\")\n",
        "ax2.set_title(\"Test\")\n",
        "ax2.axhline(float(pred_y[0]), lw=1, ls=\"--\", c=\"r\", label=\"Predicted\")\n",
        "ax2.axhspan(float(pred_y[1]), float(pred_y[2]), facecolor=\"r\", alpha=0.3)\n",
        "plot_results(results, train_x, train_y, ax1)\n",
        "plot_results(results, test_x, test_y, ax2)\n",
        "\n",
        "fig.tight_layout()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ads-course-assessment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.15 64-bit ('ads')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "ac3d72464a97ae262f31c000180823f1976a89a009e66d20ebb77defade29e57"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
